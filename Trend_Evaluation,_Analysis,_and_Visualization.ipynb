{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/liujainqi/BPB-Worker-Panel/blob/main/Trend_Evaluation%2C_Analysis%2C_and_Visualization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trend Evaluation, Analysis, and Visualization\n",
        "\n",
        "The Trend Evaluation, Analysis, and Visualization script is designed to analyze and visualize trends in news headlines and market sentiments. It performs sentiment analysis on selected news topics and market trends based on a set list of keywords relevant to the user's research. The script extracts headlines and links from RSS feeds, normalizes them in English, and applies sentiment analysis using the VADER (Valence Aware Dictionary and sEntiment Reasoner) tool. It then visualizes the sentiment scores over time to identify trends and patterns. Additionally, the script can be adapted to incorporate machine learning algorithms for risk assessment in production strategy and manufacturing. Overall, it provides valuable insights into market sentiment and news trends, enabling data-driven decision-making and strategic planning.\n",
        "\n",
        "[Lean-IQ](https://www.lean-iq.com)\n"
      ],
      "metadata": {
        "id": "m9ahSooJfzCT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dependencies**\n",
        "\n",
        "*   vaderSentiment: For sentiment analysis.\n",
        "*   feedparser: For parsing RSS feeds.eintrag\n",
        "*   googletrans: For translating text.\n",
        "*   google.colab: For mounting Google Drive (if running on Google Colab).\n",
        "\n"
      ],
      "metadata": {
        "id": "Pj27HCLEgLeR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kqrl7pRxnren",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "pip install feedparser gdown langdetect googletrans vaderSentiment googletrans==4.0.0-rc1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Features**\n",
        "\n",
        "*   Extracts news headlines and links from RSS feeds based on keywords.\n",
        "*   Normalizes news items to English.\n",
        "*   Performs sentiment analysis on news headlines using VADER sentiment analyzer.\n",
        "*   Saves results to a CSV file.\n",
        "\n",
        "\n",
        "**Configuration**\n",
        "\n",
        "*   The script requires a CSV file containing keywords. Specify the path to this file in the keywords_file_path variable.\n",
        "\n",
        "*   Results are saved to a CSV file. Specify the desired path in the csv_file_path variable.\n",
        "\n",
        "\n",
        "**Contributing**\n",
        "\n",
        "Contributions are welcome! Please fork the repository, create a new branch, and\n",
        "submit a pull request. For major changes, open an issue first to discuss potential improvements.\n",
        "\n",
        "**License**\n",
        "\n",
        "This project is licensed under the MIT License. See the LICENSE file for details. (Annex)"
      ],
      "metadata": {
        "id": "2DYLjQXNgu_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "import feedparser\n",
        "import csv\n",
        "from datetime import datetime, timedelta\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Initialize the VADER sentiment analyzer\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "def download_keywords_csv(file_path, output_path):\n",
        "    \"\"\"\n",
        "    Check if the keywords CSV file exists, if not, print a message.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path to the keywords CSV file.\n",
        "        output_path (str): The output path for the downloaded CSV file.\n",
        "\n",
        "    Returns:\n",
        "        str: The path to the downloaded CSV file if it exists, otherwise None.\n",
        "    \"\"\"\n",
        "    if os.path.exists(file_path):\n",
        "        return file_path\n",
        "    else:\n",
        "        print(\"File does not exist.\")\n",
        "        return None\n",
        "\n",
        "def extract_titles_and_links_with_date(rss_url):\n",
        "    \"\"\"\n",
        "    Extract titles, links, and dates from the given RSS feed URL.\n",
        "\n",
        "    Args:\n",
        "        rss_url (str): The URL of the RSS feed.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of tuples containing title, link, and date.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Parse the RSS feed\n",
        "        feed = feedparser.parse(rss_url)\n",
        "\n",
        "        # Extract titles, links, and dates\n",
        "        titles_links_dates = [(entry.title, entry.link, entry.published) for entry in feed.entries]\n",
        "\n",
        "        return titles_links_dates\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return None\n",
        "\n",
        "def save_to_csv(data, csv_file_path):\n",
        "    \"\"\"\n",
        "    Save data to the CSV file.\n",
        "\n",
        "    Args:\n",
        "        data (list): The data to save to the CSV file.\n",
        "        csv_file_path (str): The path to the CSV file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Open the CSV file in append mode\n",
        "        with open(csv_file_path, 'a', newline='', encoding='utf-8') as csv_file:\n",
        "            # Create a CSV writer\n",
        "            csv_writer = csv.writer(csv_file)\n",
        "\n",
        "            # Write data to CSV\n",
        "            csv_writer.writerows(data)\n",
        "\n",
        "        print(f\"Data successfully appended to: {csv_file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error appending to CSV: {e}\")\n",
        "\n",
        "def main():\n",
        "    # Specify the path to the keywords CSV file\n",
        "    keywords_file_path = '/content/drive/My Drive/DATA/keyword.csv'\n",
        "\n",
        "    # Specify the desired CSV file path in Google Drive for saving headlines\n",
        "    csv_file_path = '/content/drive/My Drive/DATA/results_sentiment.csv'\n",
        "\n",
        "    # Check if the keywords file exists\n",
        "    keywords_file_path = download_keywords_csv(keywords_file_path, '/content/keyword.csv')\n",
        "\n",
        "    # Load keywords from the downloaded CSV file\n",
        "    if keywords_file_path:\n",
        "        with open(keywords_file_path, 'r') as keywords_file:\n",
        "            reader = csv.reader(keywords_file)\n",
        "            next(reader)  # Skip the header\n",
        "            keywords = [row[0] for row in reader]\n",
        "\n",
        "        # Write headers to the CSV file\n",
        "        with open(csv_file_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
        "            csv_writer = csv.writer(csv_file)\n",
        "            csv_writer.writerow(['Keyword', 'Title', 'Date', 'Link', 'Sentiment Polarity', 'Sentiment Compound'])\n",
        "\n",
        "        # Loop through each keyword\n",
        "        for keyword in keywords:\n",
        "            # Replace spaces with '+' in the keyword for the RSS feed URL\n",
        "            keyword_for_url = keyword.replace(' ', '+')\n",
        "\n",
        "            # Construct the RSS feed URL\n",
        "            rss_feed_url = f'https://news.google.com/rss/search?q={keyword_for_url}'\n",
        "\n",
        "            # Call the function to extract titles, links, and dates\n",
        "            result = extract_titles_and_links_with_date(rss_feed_url)\n",
        "\n",
        "            if result:\n",
        "                # Filter entries that are older than 24 hours\n",
        "                current_time = datetime.now()\n",
        "                filtered_result = [(title, link, date) for title, link, date in result if (current_time - datetime.strptime(date, \"%a, %d %b %Y %H:%M:%S %Z\")).total_seconds() <= 86400]\n",
        "\n",
        "                # Prepare data for CSV with formatted date and sentiment analysis\n",
        "                data_for_csv = []\n",
        "                for title, link, date in filtered_result:\n",
        "                    sentiment_scores = analyzer.polarity_scores(title.replace('.', ','))\n",
        "                    polarity_score = f\"{sentiment_scores['pos'] - sentiment_scores['neg']:.2f}\".replace('.', ',')\n",
        "                    compound_score = f\"{sentiment_scores['compound']:.2f}\".replace('.', ',')\n",
        "                    data_for_csv.append([keyword, title, datetime.strptime(date, \"%a, %d %b %Y %H:%M:%S %Z\").strftime(\"%d.%m.%Y\"), link, polarity_score, compound_score])\n",
        "\n",
        "                # Save data to CSV\n",
        "                save_to_csv(data_for_csv, csv_file_path)\n",
        "            else:\n",
        "                print(f\"Extraction failed for keyword: {keyword}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "kmcyuY2_fN8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dU6Td9AO-E9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "i2w7raJL-FdR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Version 1.0, Prototype"
      ],
      "metadata": {
        "id": "OjNlI75Cjyao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NEWS EXTRACT - HEADLINES AND LINKS BASED ON KEYWORD LIST, ALL NORMALIZED IN ENGLISH, INCLUDING SENTIMENT ANALYSIS #\n",
        "\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "import feedparser\n",
        "import csv\n",
        "from datetime import datetime, timedelta\n",
        "from google.colab import drive\n",
        "import gdown\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Initialize the VADER sentiment analyzer\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "def download_keywords_csv(file_path, output_path):\n",
        "    # Check if the file exists\n",
        "    if os.path.exists(file_path):\n",
        "        return file_path\n",
        "    else:\n",
        "        print(\"File does not exist.\")\n",
        "        return None\n",
        "\n",
        "def extract_titles_and_links_with_date(rss_url):\n",
        "    try:\n",
        "        # Parse the RSS feed\n",
        "        feed = feedparser.parse(rss_url)\n",
        "\n",
        "        # Extract titles, links, and dates\n",
        "        titles_links_dates = [(entry.title, entry.link, entry.published) for entry in feed.entries]\n",
        "\n",
        "        return titles_links_dates\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return None\n",
        "\n",
        "def save_to_csv(data, csv_file_path):\n",
        "    try:\n",
        "        # Open the CSV file in append mode\n",
        "        with open(csv_file_path, 'a', newline='', encoding='utf-8') as csv_file:\n",
        "            # Create a CSV writer\n",
        "            csv_writer = csv.writer(csv_file)\n",
        "\n",
        "            # Write data to CSV\n",
        "            csv_writer.writerows(data)\n",
        "\n",
        "        print(f\"Data successfully appended to: {csv_file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error appending to CSV: {e}\")\n",
        "\n",
        "def main():\n",
        "    # Specify the path to the keywords CSV file\n",
        "    keywords_file_path = '/content/drive/My Drive/DATA/keyword.csv'\n",
        "\n",
        "    # Specify the desired CSV file path in Google Drive for saving headlines\n",
        "    csv_file_path = '/content/drive/My Drive/DATA/results_sentiment.csv'\n",
        "\n",
        "    # Check if the keywords file exists\n",
        "    keywords_file_path = download_keywords_csv(keywords_file_path, '/content/keyword.csv')\n",
        "\n",
        "    # Load keywords from the downloaded CSV file\n",
        "    if keywords_file_path:\n",
        "        with open(keywords_file_path, 'r') as keywords_file:\n",
        "            reader = csv.reader(keywords_file)\n",
        "            next(reader)  # Skip the header\n",
        "            keywords = [row[0] for row in reader]\n",
        "\n",
        "        # Write headers to the CSV file\n",
        "        with open(csv_file_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
        "            csv_writer = csv.writer(csv_file)\n",
        "            csv_writer.writerow(['Keyword', 'Title', 'Date', 'Link', 'Sentiment Polarity', 'Sentiment Compound'])\n",
        "\n",
        "        # Loop through each keyword\n",
        "        for keyword in keywords:\n",
        "            # Replace spaces with '+' in the keyword for the RSS feed URL\n",
        "            keyword_for_url = keyword.replace(' ', '+')\n",
        "\n",
        "            # Construct the RSS feed URL\n",
        "            rss_feed_url = f'https://news.google.com/rss/search?q={keyword_for_url}'\n",
        "\n",
        "            # Call the function to extract titles, links, and dates\n",
        "            result = extract_titles_and_links_with_date(rss_feed_url)\n",
        "\n",
        "            if result:\n",
        "                # Filter entries that are older than 24 hours\n",
        "                current_time = datetime.now()\n",
        "                filtered_result = [(title, link, date) for title, link, date in result if (current_time - datetime.strptime(date, \"%a, %d %b %Y %H:%M:%S %Z\")).total_seconds() <= 86400]\n",
        "\n",
        "                # Prepare data for CSV with formatted date and sentiment analysis\n",
        "                data_for_csv = []\n",
        "                for title, link, date in filtered_result:\n",
        "                    sentiment_scores = analyzer.polarity_scores(title.replace('.', ','))\n",
        "                    polarity_score = f\"{sentiment_scores['pos'] - sentiment_scores['neg']:.2f}\".replace('.', ',')\n",
        "                    compound_score = f\"{sentiment_scores['compound']:.2f}\".replace('.', ',')\n",
        "                    data_for_csv.append([keyword, title, datetime.strptime(date, \"%a, %d %b %Y %H:%M:%S %Z\").strftime(\"%d.%m.%Y\"), link, polarity_score, compound_score])\n",
        "\n",
        "                # Save data to CSV\n",
        "                save_to_csv(data_for_csv, csv_file_path)\n",
        "            else:\n",
        "                print(f\"Extraction failed for keyword: {keyword}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "_ky0-o8nD4hg",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization: Multidimensional Scaling (MDS)"
      ],
      "metadata": {
        "id": "honJQKXXkNzA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dependencies**\n",
        "\n",
        "*   numpy: For numerical operations.\n",
        "*   pandas: For data manipulation and analysis.\n",
        "*   vaderSentiment: For sentiment analysis.\n",
        "*   scikit-learn: For machine learning algorithms, including Multidimensional Scaling (MDS).\n",
        "*   matplotlib: For data visualization.\n",
        "*   google.colab: For mounting Google Drive (if running on Google Colab).\n",
        "*   nltk: For natural language processing tasks, including stopwords removal and lemmatization.\n",
        "\n",
        "\n",
        "**Features**\n",
        "\n",
        "*   Preprocesses sentiment results by removing stopwords and lemmatizing news headlines.\n",
        "*   Applies Multidimensional Scaling (MDS) to visualize sentiment scores in a two-dimensional space.\n",
        "\n",
        "**Configuration**\n",
        "\n",
        "Ensure the sentiment results CSV file (results_sentiment.csv) is located in Google Drive. Modify the csv_file_path variable to specify the correct path if needed.\n",
        "\n",
        "\n",
        "**Contributing**\n",
        "\n",
        "Contributions are welcome! Please fork the repository, create a new branch, and submit a pull request. For major changes, open an issue first to discuss potential improvements.\n",
        "\n",
        "**License**\n",
        "\n",
        "This project is licensed under the MIT License. See the LICENSE file for details. (Annex)"
      ],
      "metadata": {
        "id": "sLb3NlUKkTt4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from sklearn.manifold import MDS\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load the CSV file containing the sentiment results\n",
        "csv_file_path = '/content/drive/MyDrive/DATA/results_sentiment.csv'\n",
        "df = pd.read_csv(csv_file_path)\n",
        "\n",
        "# Ensure 'Title' column is present\n",
        "if 'Title' not in df.columns:\n",
        "    raise ValueError(\"The 'Title' column is not present in the DataFrame.\")\n",
        "\n",
        "# Define stopwords and initialize lemmatizer\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Preprocess the titles: remove stopwords and lemmatize\n",
        "def preprocess_text(title):\n",
        "    tokens = word_tokenize(title.lower())  # Tokenize and convert to lowercase\n",
        "    filtered_tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalnum() and token not in stop_words]  # Lemmatize and remove stopwords\n",
        "    return ' '.join(filtered_tokens)\n",
        "\n",
        "# Apply preprocessing to the 'Title' column\n",
        "df['Processed_Title'] = df['Title'].apply(preprocess_text)\n",
        "\n",
        "# Perform sentiment analysis on the preprocessed titles\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "sentiments = [analyzer.polarity_scores(title)['compound'] for title in df['Processed_Title']]\n",
        "sentiments_array = np.array(sentiments).reshape(-1, 1)\n",
        "\n",
        "# Perform Multidimensional Scaling (MDS) on sentiment scores\n",
        "mds = MDS(n_components=2, dissimilarity='euclidean', random_state=42)\n",
        "mds_transformed = mds.fit_transform(sentiments_array)\n",
        "\n",
        "# Plot the titles based on MDS transformation\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(mds_transformed[:, 0], mds_transformed[:, 1], c='blue', alpha=0.5)\n",
        "plt.title('Multidimensional Scaling (MDS) of Title Sentiments')\n",
        "plt.xlabel('MDS Dimension 1')\n",
        "plt.ylabel('MDS Dimension 2')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Va-mpJ8gk1Z8",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization: Hierarchical Clustering"
      ],
      "metadata": {
        "id": "l6q6ZFOIleO4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dependencies**\n",
        "\n",
        "\n",
        "*   numpy: For numerical operations.\n",
        "*   pandas: For data manipulation and analysis.\n",
        "*   vaderSentiment: For sentiment analysis.\n",
        "*   matplotlib: For data visualization.\n",
        "*   google.colab: For mounting Google Drive (if running on Google Colab).\n",
        "*   nltk: For natural language processing tasks, including stopwords removal and lemmatization.\n",
        "*   scikit-learn: For machine learning algorithms, including hierarchical clustering.\n",
        "\n",
        "\n",
        "**Features**\n",
        "\n",
        "\n",
        "*   Preprocesses sentiment results by removing stopwords and lemmatizing news headlines.\n",
        "*   Applies hierarchical clustering to visualize sentiment scores in a dendrogram.\n",
        "*   Identifies representative titles for each cluster.\n",
        "\n",
        "\n",
        "**Configuration**\n",
        "\n",
        "Ensure the sentiment results CSV file (results_sentiment.csv) is located in Google Drive. Modify the csv_file_path variable to specify the correct path if needed.\n",
        "\n",
        "Adjust the number of clusters (n_clusters) and the number of representative titles (n_representatives) according to your requirements.\n",
        "\n",
        "**Contributing**\n",
        "\n",
        "Contributions are welcome! Please fork the repository, create a new branch, and submit a pull request. For major changes, open an issue first to discuss potential improvements.\n",
        "\n",
        "**License**\n",
        "\n",
        "This project is licensed under the MIT License - see the LICENSE file for details. (Annex)"
      ],
      "metadata": {
        "id": "2IiTY3julpvV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load the CSV file containing the sentiment results\n",
        "csv_file_path = '/content/drive/MyDrive/DATA/results_sentiment.csv'\n",
        "df = pd.read_csv(csv_file_path)\n",
        "\n",
        "# Ensure 'Title' column is present\n",
        "if 'Title' not in df.columns:\n",
        "    raise ValueError(\"The 'Title' column is not present in the DataFrame.\")\n",
        "\n",
        "# Define stopwords and initialize lemmatizer\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Preprocess the titles: remove stopwords and lemmatize\n",
        "def preprocess_text(title):\n",
        "    tokens = word_tokenize(title.lower())  # Tokenize and convert to lowercase\n",
        "    filtered_tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalnum() and token not in stop_words]  # Lemmatize and remove stopwords\n",
        "    return ' '.join(filtered_tokens)\n",
        "\n",
        "# Apply preprocessing to the 'Title' column\n",
        "df['Processed_Title'] = df['Title'].apply(preprocess_text)\n",
        "\n",
        "# Perform sentiment analysis on the preprocessed titles\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "sentiments = [analyzer.polarity_scores(title)['compound'] for title in df['Processed_Title']]\n",
        "sentiments_array = np.array(sentiments).reshape(-1, 1)\n",
        "\n",
        "# Apply hierarchical clustering\n",
        "cluster_model = AgglomerativeClustering(n_clusters=5, affinity='euclidean', linkage='ward')\n",
        "clusters = cluster_model.fit_predict(sentiments_array)\n",
        "\n",
        "# Find representative titles for each cluster\n",
        "def representative_titles(cluster_labels, titles, n_representatives=3):\n",
        "    cluster_representatives = {}\n",
        "    for cluster_id in np.unique(cluster_labels):\n",
        "        indices = np.where(cluster_labels == cluster_id)[0]\n",
        "        cluster_sentiments = sentiments_array[indices]\n",
        "        centroid_index = np.argmin(np.mean(euclidean_distances(cluster_sentiments, cluster_sentiments), axis=0))\n",
        "        representative_indices = indices[centroid_index]\n",
        "        representative_titles = titles.iloc[[representative_indices]]\n",
        "        cluster_representatives[cluster_id] = representative_titles.tolist()[:n_representatives]\n",
        "    return cluster_representatives\n",
        "\n",
        "# Get representative titles for each cluster\n",
        "representative_clusters = representative_titles(clusters, df['Title'])\n",
        "\n",
        "# Plot dendrogram\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.title('Hierarchical Clustering Dendrogram')\n",
        "linkage_matrix = linkage(sentiments_array, method='ward')\n",
        "dendrogram(linkage_matrix, truncate_mode='level', p=3)\n",
        "plt.xlabel(\"Number of points in node (or index of point if no parenthesis).\")\n",
        "plt.ylabel(\"Distance\")\n",
        "plt.show()\n",
        "\n",
        "# Print representative titles for each cluster\n",
        "for cluster_id, titles in representative_clusters.items():\n",
        "    print(f\"Cluster {cluster_id} Titles:\")\n",
        "    for i, title in enumerate(titles, start=1):\n",
        "        print(f\"{i}. {title}\")\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "TwmMwN1lUbur",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Annex"
      ],
      "metadata": {
        "id": "_A_dR1bcibSH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## License\n",
        "\n",
        "This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details. The MIT License is a permissive license that allows you to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the software, with proper attribution and without warranty.\n"
      ],
      "metadata": {
        "id": "Ntybzi3FifBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MIT License\n",
        "\n",
        "# Copyright (c) [2024] [Lean-IQ, Ralf Puehler]\n",
        "\n",
        "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "# of this software and associated documentation files (the \"Software\"), to deal\n",
        "# in the Software without restriction, including without limitation the rights\n",
        "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "# copies of the Software, and to permit persons to whom the Software is\n",
        "# furnished to do so, subject to the following conditions:\n",
        "\n",
        "# The above copyright notice and this permission notice shall be included in all\n",
        "# copies or substantial portions of the Software.\n",
        "\n",
        "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "#FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "# SOFTWARE."
      ],
      "metadata": {
        "id": "YGqwJ9najAkn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}